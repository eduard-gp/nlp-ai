{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "523041b7-fcea-4f21-ae42-94d64dbbd1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score, confusion_matrix\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "import preprocess_utils as prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1c4d8544-21e2-4804-81b3-6b035e37801d",
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 20_000\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "\n",
    "def to_lower(tokens):\n",
    "    lower_tokens = []\n",
    "    for sentence in tokens:\n",
    "        lower_sentence = []\n",
    "        for word in sentence:\n",
    "            lower_sentence.append(word.lower())\n",
    "        lower_tokens.append(lower_sentence)\n",
    "    return lower_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "15c9fc73-8549-4ad3-880b-d6e4960e1780",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path = os.path.join(\"datasets\", \"conll2003\")\n",
    "\n",
    "train_data = prep.read_iob_file(os.path.join(dir_path, \"train.txt\"))\n",
    "valid_data = prep.read_iob_file(os.path.join(dir_path, \"valid.txt\"))\n",
    "test_data = prep.read_iob_file(os.path.join(dir_path, \"test.txt\"))\n",
    "\n",
    "train_data[\"tokens\"] = to_lower(train_data[\"tokens\"])\n",
    "x_train, vocab = prep.preprocess_tokens(train_data[\"tokens\"], VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
    "y_train, class_names = prep.preprocess_entity_tags(train_data[\"entity_tags\"], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "valid_data[\"tokens\"] = to_lower(valid_data[\"tokens\"])\n",
    "x_valid, _ = prep.preprocess_tokens(valid_data[\"tokens\"], VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
    "y_valid, _ = prep.preprocess_entity_tags(valid_data[\"entity_tags\"], MAX_SEQUENCE_LENGTH)\n",
    "\n",
    "test_data[\"tokens\"] = to_lower(test_data[\"tokens\"])\n",
    "x_test, _ = prep.preprocess_tokens(test_data[\"tokens\"], VOCAB_SIZE, MAX_SEQUENCE_LENGTH)\n",
    "y_test, _ = prep.preprocess_entity_tags(test_data[\"entity_tags\"], MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ee03835c-f207-4123-b212-8cb9b64dd591",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = (tf.data.Dataset\n",
    "                 .from_tensor_slices((x_train, y_train))\n",
    "                 .batch(batch_size))\n",
    "\n",
    "valid_dataset = (tf.data.Dataset\n",
    "                 .from_tensor_slices((x_valid, y_valid))\n",
    "                 .batch(batch_size))\n",
    "\n",
    "test_dataset = (tf.data.Dataset\n",
    "                .from_tensor_slices((x_test, y_test))\n",
    "                .batch(batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eaa56075-a4b0-458f-992a-417035a27abe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train examples:      14041, vocabulary length: 20000, classes: 9\n",
      "Validation examples: 3250\n",
      "Test examples:       3453\n",
      "Labels:              ['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train examples:      {x_train.shape[0]}, vocabulary length: {len(vocab)}, classes: {len(class_names)}\")\n",
    "print(f\"Validation examples: {x_valid.shape[0]}\")\n",
    "print(f\"Test examples:       {x_test.shape[0]}\")\n",
    "print(f\"Labels:              {class_names}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d8be37b-1ad4-4cae-9b5b-ed384d6b17c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.']\n",
      "['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train_data[\"tokens\"][0])\n",
    "print(train_data[\"entity_tags\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b549ca1-6f9f-43e4-a707-79af65cfd6e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  989 10951   205   629     7  3939   216  5774     3     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0]\n",
      "[3 9 2 9 9 9 2 9 9 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(x_train[0])\n",
    "print(y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "74641fbe-e5f5-46e5-bf39-346db24232a1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PAD] : 0\n",
      "[UNK] : 1\n",
      "the : 2\n",
      ". : 3\n",
      ", : 4\n",
      "of : 5\n",
      "in : 6\n",
      "to : 7\n",
      "a : 8\n",
      "and : 9\n"
     ]
    }
   ],
   "source": [
    "# vocabulary of how the tokens were mapped to integers\n",
    "# [PAD] is for padding when the sequence of words is less than MAXMAX_SEQUENCE_LENGTH\n",
    "# [UNK] is for unknown words (words that are not in the vocabulary)\n",
    "for k, v in list(vocab.items())[:10]:\n",
    "    print(f\"{k} : {v}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95d1b994-a7de-4c9a-b027-ae9587e71833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, None, 512)        1050624   \n",
      " l)                                                              \n",
      "                                                                 \n",
      " bidirectional_1 (Bidirectio  (None, None, 512)        1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " bidirectional_2 (Bidirectio  (None, None, 512)        1574912   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " time_distributed (TimeDistr  (None, None, 10)         5130      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 9,325,578\n",
      "Trainable params: 9,325,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "units = 256\n",
    "embedding_dim = 256\n",
    "\n",
    "inputs = layers.Input(shape=(None,))\n",
    "x = layers.Embedding(VOCAB_SIZE, embedding_dim, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(units,\n",
    "                                     return_sequences=True,\n",
    "                                     kernel_regularizer=tf.keras.regularizers.L2(0.001),\n",
    "                                     dropout=0.1))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units,\n",
    "                                     return_sequences=True,\n",
    "                                     kernel_regularizer=tf.keras.regularizers.L2(0.001),\n",
    "                                     dropout=0.1))(x)\n",
    "x = layers.Bidirectional(layers.LSTM(units,\n",
    "                                     return_sequences=True,\n",
    "                                     kernel_regularizer=tf.keras.regularizers.L2(0.001),\n",
    "                                     dropout=0.1))(x)\n",
    "outputs = layers.TimeDistributed(layers.Dense(len(class_names) + 1))(x)\n",
    "\n",
    "model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0217e827-2caa-44cb-850a-6969029d86d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IgnorePaddingSparseCategoricalCrossentropy(tf.keras.losses.Loss):\n",
    "    def __init__(self, from_logits=False, weight_class=None):\n",
    "        super(IgnorePaddingSparseCategoricalCrossentropy, self).__init__()\n",
    "        self.from_logits = from_logits\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=self.from_logits,\n",
    "            reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "        self.weight_class = weight_class\n",
    "    \n",
    "    def call(self, y_true, y_pred, class_weight=None):\n",
    "        loss = self.loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast(tf.not_equal(y_true, 0), dtype=tf.dtypes.float32)\n",
    "        if self.weight_class is not None:\n",
    "            weights = tf.gather(self.weight_class, y_true)\n",
    "            result = mask * loss * weights\n",
    "        else:\n",
    "            result = mask * loss\n",
    "        return tf.reduce_sum(result) / tf.reduce_sum(mask)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"from_logits\": self.from_logits\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "37c234ab-405b-4d85-b232-0d745059690b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IgnorePaddingSparseCategoricalAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self):\n",
    "        super(IgnorePaddingSparseCategoricalAccuracy, self).__init__(name=\"accuracy\")\n",
    "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "    \n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        labels = tf.math.argmax(y_pred, axis=2)\n",
    "        mask = tf.not_equal(y_true, 0)\n",
    "        correct_predictions = tf.equal(y_true, labels)\n",
    "        correct_predictions = tf.cast(tf.logical_and(mask, correct_predictions),\n",
    "                                      dtype=tf.dtypes.float32)\n",
    "        total_labels = tf.cast(mask, dtype=tf.dtypes.float32)\n",
    "        self.count.assign_add(tf.reduce_sum(correct_predictions))\n",
    "        self.total.assign_add(tf.reduce_sum(total_labels))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.count / self.total\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total.assign(0.0)\n",
    "        self.count.assign(0.0)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f3f511d-3808-4769-a385-8de4c711cf6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[True, True, True, True, True, True]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[layer.supports_masking for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b5d1428b-40b3-45d4-bf67-c17243674df9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-LOC', 'B-MISC', 'B-ORG', 'B-PER', 'I-LOC', 'I-MISC', 'I-ORG', 'I-PER', 'O']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2a680fc-a75a-472a-be19-fdf307081688",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_dict = {\n",
    "    0: 1.0,\n",
    "    1: 5.0,\n",
    "    2: 5.0,\n",
    "    3: 5.0,\n",
    "    4: 5.0,\n",
    "    5: 5.0,\n",
    "    6: 5.0,\n",
    "    7: 5.0,\n",
    "    8: 5.0,\n",
    "    9: 1.0\n",
    "}\n",
    "\n",
    "class_weight = tf.constant(list(class_weight_dict.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95ccf886-a25d-46f9-88f7-86af2f7d4e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-6),\n",
    "    loss=IgnorePaddingSparseCategoricalCrossentropy(from_logits=True, weight_class=class_weight),\n",
    "    metrics=[IgnorePaddingSparseCategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "# model.compile(\n",
    "#     optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "#     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "#     metrics=[\"accuracy\"]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "596bb1d0-e3bd-4cff-a4f2-231b72329b98",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "439/439 [==============================] - 118s 223ms/step - loss: 7.8034 - accuracy: 0.6092 - val_loss: 5.3463 - val_accuracy: 0.8309\n",
      "Epoch 2/20\n",
      "439/439 [==============================] - 187s 426ms/step - loss: 7.7107 - accuracy: 0.8296 - val_loss: 5.2648 - val_accuracy: 0.8300\n",
      "Epoch 3/20\n",
      "439/439 [==============================] - 238s 542ms/step - loss: 7.5896 - accuracy: 0.8295 - val_loss: 5.1618 - val_accuracy: 0.8279\n",
      "Epoch 4/20\n",
      "439/439 [==============================] - 238s 542ms/step - loss: 7.3615 - accuracy: 0.8296 - val_loss: 4.9665 - val_accuracy: 0.8310\n",
      "Epoch 5/20\n",
      "439/439 [==============================] - 238s 542ms/step - loss: 6.9208 - accuracy: 0.8298 - val_loss: 4.7400 - val_accuracy: 0.8305\n",
      "Epoch 6/20\n",
      "439/439 [==============================] - 238s 543ms/step - loss: 6.7240 - accuracy: 0.8299 - val_loss: 4.6811 - val_accuracy: 0.8305\n",
      "Epoch 7/20\n",
      "439/439 [==============================] - 238s 542ms/step - loss: 6.6554 - accuracy: 0.8299 - val_loss: 4.6262 - val_accuracy: 0.8306\n",
      "Epoch 8/20\n",
      "439/439 [==============================] - 239s 544ms/step - loss: 6.5864 - accuracy: 0.8299 - val_loss: 4.5714 - val_accuracy: 0.8307\n",
      "Epoch 9/20\n",
      "439/439 [==============================] - 239s 544ms/step - loss: 6.5233 - accuracy: 0.8299 - val_loss: 4.5181 - val_accuracy: 0.8312\n",
      "Epoch 10/20\n",
      "439/439 [==============================] - 239s 543ms/step - loss: 6.4595 - accuracy: 0.8297 - val_loss: 4.4686 - val_accuracy: 0.8309\n",
      "Epoch 11/20\n",
      "110/439 [======>.......................] - ETA: 2:48 - loss: 6.3103 - accuracy: 0.8202"
     ]
    }
   ],
   "source": [
    "epochs = 20\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    validation_data=valid_dataset,\n",
    "    class_weight=class_weight_dict\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf96207-5d0d-48ae-9d9d-bede6b91ddbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = model.evaluate(test_dataset)\n",
    "\n",
    "for metric_name, metric in zip(model.metrics_names, metrics):\n",
    "    print(f\"{metric_name:>7s} {metric:4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae18f94-75e5-42b4-a5bb-291f8b336e74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6), constrained_layout=True)\n",
    "ax.plot(history.history[\"loss\"], label=\"Training loss\")\n",
    "ax.plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax.set_xlabel(\"Epcohs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xticks(np.arange(len(history.history[\"loss\"])))\n",
    "ax.legend()\n",
    "# plt.savefig(\"loss.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb2da6c-66f3-41a6-9bc8-283c5d7c1d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6), constrained_layout=True)\n",
    "ax.plot(history.history[\"accuracy\"], label=\"Training accuracy\")\n",
    "ax.plot(history.history[\"val_accuracy\"], label=\"Validation accuracy\")\n",
    "ax.set_xlabel(\"Epcohs\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_xticks(np.arange(len(history.history[\"accuracy\"])))\n",
    "ax.legend()\n",
    "# fig.savefig(\"accuracy.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "699227cb-3085-4afe-b0fc-9dab090a2ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for tokens, labels in test_dataset:\n",
    "    y_true.append(labels.numpy())\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "predictions = model.predict(test_dataset)\n",
    "y_pred = np.argmax(predictions, axis=2)\n",
    "\n",
    "y_pred_raveled = y_pred[y_true != 0]\n",
    "y_true_raveled = y_true[y_true != 0]\n",
    "\n",
    "accuracy = accuracy_score(y_true_raveled, y_pred_raveled)\n",
    "precision = precision_score(y_true_raveled, y_pred_raveled, average=\"macro\")\n",
    "recall = recall_score(y_true_raveled, y_pred_raveled, average=\"macro\")\n",
    "f1score = f1_score(y_true_raveled, y_pred_raveled, average=\"macro\")\n",
    "print(f\"accuracy : {accuracy:.4f}\")\n",
    "print(f\"precision: {precision:.4f}\")\n",
    "print(f\"recall   : {recall:.4f}\")\n",
    "print(f\"F1       : {f1score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4389f15-fea7-47d3-b030-1241d65d87fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix = confusion_matrix(y_true_raveled, y_pred_raveled)\n",
    "df = pd.DataFrame(matrix, index=class_names, columns=class_names)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(df, annot=True, fmt=\"d\")\n",
    "# plt.savefig(\"confusion_matrix.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52894e26-b643-4282-ab9c-782a1e7031f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = []\n",
    "for tokens, labels in train_dataset:\n",
    "    y_true.append(labels.numpy())\n",
    "y_true = np.concatenate(y_true, axis=0)\n",
    "\n",
    "predictions = model.predict(train_dataset)\n",
    "y_pred = np.argmax(predictions, axis=2)\n",
    "\n",
    "y_pred_raveled = y_pred[y_true != 0]\n",
    "y_true_raveled = y_true[y_true != 0]\n",
    "\n",
    "matrix = confusion_matrix(y_true_raveled, y_pred_raveled)\n",
    "df = pd.DataFrame(matrix, index=class_names, columns=class_names)\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(df, annot=True, fmt=\"d\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c300acd-1390-4d8e-8ed9-0916a741aa01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# At prediction time it is necessary to work on a list of strings\n",
    "class PreprocessTextLayer(layers.Layer):\n",
    "    def __init__(self, vocabulary, max_sequence):\n",
    "        super(PreprocessTextLayer, self).__init__()\n",
    "        self.text_vectorization_layer = layers.TextVectorization(\n",
    "            vocabulary=vocabulary,\n",
    "            standardize=\"lower\"\n",
    "        )\n",
    "        self.max_sequence = max_sequence\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        transformed_inputs = self.text_vectorization_layer(inputs)\n",
    "        n = transformed_inputs.shape[1]\n",
    "        if n > self.max_sequence:\n",
    "            transformed_inputs = transformed_inputs[:, self.max_sequence]\n",
    "        else:\n",
    "            transformed_inputs = tf.pad(transformed_inputs, [[0, 0], [0, self.max_sequence - n]])\n",
    "        return transformed_inputs\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"vocabulary\": self.vocabulary,\n",
    "            \"max_sequence\": self.max_sequence\n",
    "        })\n",
    "        return config"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "licenta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
