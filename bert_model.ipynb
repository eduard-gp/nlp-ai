{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "deb91735-0006-428f-8bdd-7241b317f384",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_text as text\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "import preprocess_utils as pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7e7ff4f-44d7-4dcb-b0ce-581869e9810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conll2003 dataset features: {'pos_tags', 'chunk_tags', 'tokens', 'entity_tags'}\n"
     ]
    }
   ],
   "source": [
    "data = pre.read_iob_file(os.path.join(\"conll2003_data\", \"train.txt\"))\n",
    "print(f\"Conll2003 dataset features: {set(data.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f40594-26b0-44c2-822b-5ef714b61719",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 14018\n"
     ]
    }
   ],
   "source": [
    "train_text = data[\"tokens\"]\n",
    "train_labels = data[\"entity_tags\"]\n",
    "print(f\"Train dataset: {len(train_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bce992f8-8fdb-4285-b18a-696d2ea55617",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataset: 3242\n"
     ]
    }
   ],
   "source": [
    "data = pre.read_iob_file(os.path.join(\"conll2003_data\", \"valid.txt\"))\n",
    "valid_text = data[\"tokens\"]\n",
    "valid_labels = data[\"entity_tags\"]\n",
    "print(f\"Validation dataset: {len(valid_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13dcf43d-1d6a-4bea-9ff2-a6ef284a4ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset: 3450\n"
     ]
    }
   ],
   "source": [
    "data = pre.read_iob_file(os.path.join(\"conll2003_data\", \"test.txt\"))\n",
    "test_text = data[\"tokens\"]\n",
    "test_labels = data[\"entity_tags\"]\n",
    "print(f\"Test dataset: {len(test_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b9b9a64-7785-441d-b479-233308d2e9a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: ['EU', 'rejects', 'German', 'call', 'to', 'boycott', 'British', 'lamb']\n",
      "Labels:   ['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(f\"Sentence: {train_text[0]}\")\n",
    "print(f\"Labels:   {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cc2de9c-6c80-4c3f-bbd3-cb1c843b3c4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'B-LOC': 1,\n",
       " 'B-MISC': 2,\n",
       " 'B-ORG': 3,\n",
       " 'B-PER': 4,\n",
       " 'I-LOC': 5,\n",
       " 'I-MISC': 6,\n",
       " 'I-ORG': 7,\n",
       " 'I-PER': 8,\n",
       " 'O': 9}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_unique_labels(labels):\n",
    "    unique_labels = set()\n",
    "    for sentence_labels in labels:\n",
    "        unique_labels.update(sentence_labels)\n",
    "    return unique_labels\n",
    "\n",
    "unique_labels = sorted(get_unique_labels(train_labels))\n",
    "label_to_idx = {label: idx+1 for idx, label in enumerate(unique_labels)}\n",
    "idx_to_label = {idx+1: label for idx, label in enumerate(unique_labels)}\n",
    "\n",
    "label_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bef981e1-9301-4267-b80f-b0ad2f905a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 30\n",
    "\n",
    "preprocessor = hub.load(\"https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3\")\n",
    "tokenizer = hub.KerasLayer(preprocessor.tokenize)\n",
    "packer = hub.KerasLayer(\n",
    "    preprocessor.bert_pack_inputs,\n",
    "    arguments=dict(seq_length=MAX_SEQ_LENGTH)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7358e5cc-be5c-4953-8141-e1b25a35f469",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Special tokens:\n",
      "    padding_id          : 0\n",
      "    end_of_segment_id   : 102\n",
      "    vocab_size          : 30522\n",
      "    mask_id             : 103\n",
      "    start_of_sequence_id: 101\n"
     ]
    }
   ],
   "source": [
    "special_tokens = preprocessor.tokenize.get_special_tokens_dict()\n",
    "print(\"Special tokens:\")\n",
    "for key, value in special_tokens.items():\n",
    "    print(f\"    {key:20}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c063eb1a-4714-43a5-a4aa-ef6926f5bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKEN_START = special_tokens[\"start_of_sequence_id\"]\n",
    "TOKEN_END = special_tokens[\"end_of_segment_id\"]\n",
    "TOKEN_PAD = special_tokens[\"padding_id\"]\n",
    "\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb08e75f-eca4-4b89-bd2a-dd40605878c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the last two dimensions of the tokens and\n",
    "# compute in how many subtoken a word was divided\n",
    "def merge_dims_and_get_tokens_length(tokens):\n",
    "    tokens = tokens.merge_dims(-2, -1)\n",
    "    num_subtokens_per_token = tf.map_fn(lambda token: tf.size(token), tokens, fn_output_signature=tf.int32)\n",
    "    return tokens, num_subtokens_per_token\n",
    "\n",
    "# Build a lookup table for labels\n",
    "init = tf.lookup.KeyValueTensorInitializer(\n",
    "    keys=unique_labels,\n",
    "    values=tf.range(1, len(unique_labels) + 1, dtype=tf.int64)\n",
    ")\n",
    "table = tf.lookup.StaticVocabularyTable(init, num_oov_buckets=1)\n",
    "\n",
    "def preprocess_dataset(tokens_info, labels):\n",
    "    tokens, num_subtokens_per_token = tokens_info\n",
    "    \n",
    "    # Bert packer works on batches so a new dimension is necessary\n",
    "    tokens = tf.expand_dims(tokens, axis=0)\n",
    "    packed_tokens = packer([tokens])\n",
    "    # The bert packer already has the output as a batch. This is necessary because\n",
    "    # we are preprocessing a list of string words (sentence) instead of a list of\n",
    "    # string sentences. Thus if we want to batch this sentences we have to reshape.\n",
    "    packed_tokens[\"input_word_ids\"] = tf.reshape(packed_tokens[\"input_word_ids\"], (-1,))\n",
    "    packed_tokens[\"input_type_ids\"] = tf.reshape(packed_tokens[\"input_type_ids\"], (-1,))\n",
    "    packed_tokens[\"input_mask\"] = tf.reshape(packed_tokens[\"input_mask\"], (-1,))\n",
    "    \n",
    "    # Convert labels to ids and aligne them to the number of subtokens\n",
    "    labels_ids = table.lookup(labels)\n",
    "    aligned_labels_ids = tf.repeat(labels_ids, num_subtokens_per_token)\n",
    "    # The first token in packed tokens is TOKEN_START and the last is TOKEN_END\n",
    "    # Also the packed tokens are padded to MAX_SEQUENCE_LENGTH\n",
    "    logical_pos = tf.logical_and(\n",
    "        tf.not_equal(packed_tokens[\"input_word_ids\"], TOKEN_START),\n",
    "        tf.not_equal(packed_tokens[\"input_word_ids\"], TOKEN_END)\n",
    "    )\n",
    "    logical_pos = tf.logical_and(\n",
    "        logical_pos,\n",
    "        tf.not_equal(packed_tokens[\"input_word_ids\"], TOKEN_PAD)\n",
    "    )\n",
    "    \n",
    "    shape = packed_tokens[\"input_word_ids\"].shape\n",
    "    # When the labels are of length MAX_SEMAX_SEQ_LENGTH ingnore the last\n",
    "    # two labels because they are also discarded by the bert packer in favor\n",
    "    # of the START_TOKEN and END_TOKEN\n",
    "    labels_end_index = MAX_SEQ_LENGTH - 2\n",
    "    aligned_labels_ids = tf.scatter_nd(\n",
    "        tf.where(logical_pos),\n",
    "        aligned_labels_ids[:labels_end_index],\n",
    "        shape\n",
    "    )\n",
    "    return packed_tokens, aligned_labels_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7cb54e42-24ae-44a5-9f91-659354360afd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Train dataset\n",
    "train_text_ragged_tensors = tf.ragged.constant(train_text)\n",
    "train_labels_ragged_tensors = tf.ragged.constant(train_labels)\n",
    "\n",
    "train_text_dataset = (tf.data.Dataset.from_tensor_slices(train_text_ragged_tensors)\n",
    "                .map(lambda text: tokenizer(text))\n",
    "                .map(merge_dims_and_get_tokens_length)\n",
    "               )\n",
    "\n",
    "train_labels_dataset = tf.data.Dataset.from_tensor_slices(train_labels_ragged_tensors)\n",
    "\n",
    "train_dataset = (tf.data.Dataset.zip((train_text_dataset, train_labels_dataset))\n",
    "                 .map(preprocess_dataset)\n",
    "                 .batch(batch_size)\n",
    "                 .cache()\n",
    "                )\n",
    "\n",
    "# Validation dataset\n",
    "valid_text_ragged_tensors = tf.ragged.constant(valid_text)\n",
    "valid_labels_ragged_tensors = tf.ragged.constant(valid_labels)\n",
    "\n",
    "valid_text_dataset = (tf.data.Dataset.from_tensor_slices(valid_text_ragged_tensors)\n",
    "                .map(lambda text: tokenizer(text))\n",
    "                .map(merge_dims_and_get_tokens_length)\n",
    "               )\n",
    "\n",
    "valid_labels_dataset = tf.data.Dataset.from_tensor_slices(valid_labels_ragged_tensors)\n",
    "\n",
    "valid_dataset = (tf.data.Dataset.zip((valid_text_dataset, valid_labels_dataset))\n",
    "                 .map(preprocess_dataset)\n",
    "                 .batch(batch_size)\n",
    "                 .cache()\n",
    "                )\n",
    "\n",
    "# Test dataset\n",
    "test_text_ragged_tensors = tf.ragged.constant(test_text)\n",
    "test_labels_ragged_tensors = tf.ragged.constant(test_labels)\n",
    "\n",
    "test_text_dataset = (tf.data.Dataset.from_tensor_slices(test_text_ragged_tensors)\n",
    "                .map(lambda text: tokenizer(text))\n",
    "                .map(merge_dims_and_get_tokens_length)\n",
    "               )\n",
    "\n",
    "test_labels_dataset = tf.data.Dataset.from_tensor_slices(test_labels_ragged_tensors)\n",
    "\n",
    "test_dataset = (tf.data.Dataset.zip((test_text_dataset, test_labels_dataset))\n",
    "                 .map(preprocess_dataset)\n",
    "                 .batch(batch_size)\n",
    "                 .cache()\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab2ed3ec-0b17-491f-8681-e108a20057a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IgnorePaddingSparseCategoricalCrossentropyLoss(tf.keras.losses.Loss):\n",
    "    def __init__(self, from_logits=False, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "            from_logits=from_logits,\n",
    "            reduction=tf.keras.losses.Reduction.NONE\n",
    "        )\n",
    "    \n",
    "    def call(self, y_true, y_pred):\n",
    "        loss = self.loss_fn(y_true, y_pred)\n",
    "        mask = tf.cast(tf.not_equal(y_true,  0), dtype=tf.dtypes.float32)\n",
    "        loss = loss * mask\n",
    "        return tf.reduce_sum(loss) / tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "361b3ac3-f900-48d7-9d20-d199b3c5577e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IgnorePaddingSparseCategoricalAccuracy(tf.keras.metrics.Metric):\n",
    "    def __init__(self, name=\"accuracy\", **kwargs):\n",
    "        super(IgnorePaddingSparseCategoricalAccuracy, self).__init__(name=name, **kwargs)\n",
    "        self.total = self.add_weight(name=\"total\", initializer=\"zeros\")\n",
    "        self.count = self.add_weight(name=\"count\", initializer=\"zeros\")\n",
    "\n",
    "    def update_state(self, y_true, y_pred, sample_weight=None):\n",
    "        labels = tf.math.argmax(y_pred, axis=2)\n",
    "        # labels = tf.cast(y_pred, dtype=tf.dtypes.int64)\n",
    "        mask = tf.not_equal(y_true, 0)\n",
    "        correct_predictions = tf.equal(y_true, labels)\n",
    "        correct_predictions = tf.cast(tf.logical_and(mask, correct_predictions),\n",
    "                                      dtype=tf.dtypes.float32)\n",
    "        total_labels = tf.cast(mask, dtype=tf.dtypes.float32)\n",
    "        self.count.assign_add(tf.reduce_sum(correct_predictions))\n",
    "        self.total.assign_add(tf.reduce_sum(total_labels))\n",
    "    \n",
    "    def result(self):\n",
    "        return self.count / self.total\n",
    "    \n",
    "    def reset_state(self):\n",
    "        self.total.assign(0.0)\n",
    "        self.count.assign(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9aef9e7d-81a9-4784-8259-e77ca54549da",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = hub.KerasLayer(\n",
    "    \"https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/2\",\n",
    "    trainable=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6aa59c80-838a-459d-99d3-d16d544982f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = dict(\n",
    "    input_word_ids=tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32),\n",
    "    input_mask=tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32),\n",
    "    input_type_ids=tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype=tf.int32),\n",
    ")\n",
    "encoder_outputs = encoder(encoder_inputs)\n",
    "outputs = tf.keras.layers.TimeDistributed(tf.keras.layers.Dense(len(unique_labels) + 1))(encoder_outputs[\"sequence_output\"])\n",
    "\n",
    "model = tf.keras.Model(inputs=encoder_inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c631027a-d60b-4ba9-ab82-2ffa40ed9917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " input_1 (InputLayer)           [(None, 30)]         0           []                               \n",
      "                                                                                                  \n",
      " keras_layer_2 (KerasLayer)     {'sequence_output':  11170561    ['input_2[0][0]',                \n",
      "                                 (None, 30, 256),                 'input_3[0][0]',                \n",
      "                                 'pooled_output': (               'input_1[0][0]']                \n",
      "                                None, 256),                                                       \n",
      "                                 'encoder_outputs':                                               \n",
      "                                 [(None, 30, 256),                                                \n",
      "                                 (None, 30, 256),                                                 \n",
      "                                 (None, 30, 256),                                                 \n",
      "                                 (None, 30, 256)],                                                \n",
      "                                 'default': (None,                                                \n",
      "                                256)}                                                             \n",
      "                                                                                                  \n",
      " time_distributed (TimeDistribu  (None, 30, 10)      2570        ['keras_layer_2[0][6]']          \n",
      " ted)                                                                                             \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,173,131\n",
      "Trainable params: 11,173,130\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d6e3e627-02f8-4cd5-ad48-6d7535f1ced7",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "train_data_size = len(train_text)\n",
    "steps_per_epoch = int(train_data_size / batch_size)\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "warmup_steps = int(0.1 * num_train_steps)\n",
    "initial_learning_rate = 2e-5\n",
    "\n",
    "linear_decay = tf.keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate=2e-5,\n",
    "    end_learning_rate=0,\n",
    "    decay_steps=num_train_steps\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(2e-5),\n",
    "    loss=IgnorePaddingSparseCategoricalCrossentropyLoss(from_logits=True),\n",
    "    metrics=[IgnorePaddingSparseCategoricalAccuracy()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0e52fc-0c7d-447b-b104-597d0e89282f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "132/439 [========>.....................] - ETA: 2:59 - loss: 0.9568 - accuracy: 0.7449"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=epochs,\n",
    "    callbacks=[tf.keras.callbacks.TensorBoard(log_dir=\"logs\", histogram_freq=1)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7c7a98d-ee0d-4c8d-9b78-70991241935e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6), layout=\"constrained\")\n",
    "\n",
    "ax.plot(history.history[\"loss\"], label=\"Training loss\")\n",
    "ax.plot(history.history[\"val_loss\"], label=\"Validation loss\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97cb27e1-f932-40d4-b591-22b77d3ca646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "ner_model = tf.keras.models.load_model(\"ner_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licenta",
   "language": "python",
   "name": "licenta"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
